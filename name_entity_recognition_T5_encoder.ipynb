{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **NER with T5 Encoder**","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets sentencepiece accelerate peft -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers import T5Tokenizer, T5EncoderModel\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorForTokenClassification, AdamW\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:41:44.793311Z","iopub.execute_input":"2025-01-14T08:41:44.793985Z","iopub.status.idle":"2025-01-14T08:41:44.798442Z","shell.execute_reply.started":"2025-01-14T08:41:44.793951Z","shell.execute_reply":"2025-01-14T08:41:44.797432Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"checkpoint = \"t5-small\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:17.886125Z","iopub.execute_input":"2025-01-14T08:40:17.886592Z","iopub.status.idle":"2025-01-14T08:40:18.281480Z","shell.execute_reply.started":"2025-01-14T08:40:17.886562Z","shell.execute_reply":"2025-01-14T08:40:18.280544Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"my_checkpoint = \"danfarh2000/text-summarization-T5\"\nmodel.load_adapter(my_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:18.285093Z","iopub.execute_input":"2025-01-14T08:40:18.285327Z","iopub.status.idle":"2025-01-14T08:40:18.805473Z","shell.execute_reply.started":"2025-01-14T08:40:18.285306Z","shell.execute_reply":"2025-01-14T08:40:18.804841Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(my_checkpoint)\nencoder = T5EncoderModel.from_pretrained(my_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:19.323080Z","iopub.execute_input":"2025-01-14T08:40:19.323338Z","iopub.status.idle":"2025-01-14T08:40:20.181208Z","shell.execute_reply.started":"2025-01-14T08:40:19.323317Z","shell.execute_reply":"2025-01-14T08:40:20.180328Z"}},"outputs":[{"name":"stderr","text":"Loading adapter weights from danfarh2000/text-summarization-T5 led to unexpected keys not found in the model:  ['decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight']. \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"conll2003\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:20.182379Z","iopub.execute_input":"2025-01-14T08:40:20.182615Z","iopub.status.idle":"2025-01-14T08:40:21.516683Z","shell.execute_reply.started":"2025-01-14T08:40:20.182594Z","shell.execute_reply":"2025-01-14T08:40:21.515925Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:21.517668Z","iopub.execute_input":"2025-01-14T08:40:21.518064Z","iopub.status.idle":"2025-01-14T08:40:21.523481Z","shell.execute_reply.started":"2025-01-14T08:40:21.518038Z","shell.execute_reply":"2025-01-14T08:40:21.522822Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3453\n    })\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:23.142943Z","iopub.execute_input":"2025-01-14T08:40:23.143233Z","iopub.status.idle":"2025-01-14T08:40:23.148310Z","shell.execute_reply.started":"2025-01-14T08:40:23.143213Z","shell.execute_reply":"2025-01-14T08:40:23.147313Z"}},"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"label_map = {\n    0: \"O\", # Outside of a named entity\n    1: \"B-PER\", # Beginning of a person entity\n    2: \"I-PER\", # Inside a person entity\n    3: \"B-ORG\", # Beginning of an organization entity\n    4: \"I-ORG\", # Inside an organization entity\n    5: \"B-LOC\", # Beginning of a location entity\n    6: \"I-LOC\", # Inside a location entity\n    7: \"B-MISC\", # Beginning of a miscellaneous entity\n    8: \"I-MISC\" # Inside a miscellaneous entity\n}\n\nnum_labels = len(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:23.444283Z","iopub.execute_input":"2025-01-14T08:40:23.444543Z","iopub.status.idle":"2025-01-14T08:40:23.448616Z","shell.execute_reply.started":"2025-01-14T08:40:23.444522Z","shell.execute_reply":"2025-01-14T08:40:23.447702Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:23.625316Z","iopub.execute_input":"2025-01-14T08:40:23.625537Z","iopub.status.idle":"2025-01-14T08:40:23.630377Z","shell.execute_reply.started":"2025-01-14T08:40:23.625517Z","shell.execute_reply":"2025-01-14T08:40:23.629514Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"9"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        # Get the word IDs for the tokenized input\n        word_ids = []\n        for word_idx in tokenized_inputs.word_ids(i):\n            if word_idx is not None:\n                word_ids.append(word_idx)\n            else:\n                word_ids.append(-100)  # Special token (e.g., [CLS], [SEP], [PAD])\n\n        # Align labels with tokens\n        label_ids = []\n        current_word_idx = None\n        for word_idx in word_ids:\n            if word_idx == -100:\n                label_ids.append(-100)  # Special token, ignore in loss\n            elif word_idx != current_word_idx:\n                # New word, assign the corresponding label\n                label_ids.append(label[word_idx])\n                current_word_idx = word_idx\n            else:\n                # Same word, assign -100 (ignore in loss)\n                label_ids.append(-100)\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:25.963612Z","iopub.execute_input":"2025-01-14T08:40:25.963980Z","iopub.status.idle":"2025-01-14T08:40:25.969820Z","shell.execute_reply.started":"2025-01-14T08:40:25.963950Z","shell.execute_reply":"2025-01-14T08:40:25.968905Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:26.172847Z","iopub.execute_input":"2025-01-14T08:40:26.173089Z","iopub.status.idle":"2025-01-14T08:40:26.674862Z","shell.execute_reply.started":"2025-01-14T08:40:26.173067Z","shell.execute_reply":"2025-01-14T08:40:26.674249Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f933a6d4d06b4c32b951032ac6d1fa3b"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:28.294441Z","iopub.execute_input":"2025-01-14T08:40:28.294735Z","iopub.status.idle":"2025-01-14T08:40:28.300051Z","shell.execute_reply.started":"2025-01-14T08:40:28.294710Z","shell.execute_reply":"2025-01-14T08:40:28.299055Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 3453\n    })\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.remove_columns(['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:28.542526Z","iopub.execute_input":"2025-01-14T08:40:28.542808Z","iopub.status.idle":"2025-01-14T08:40:28.551442Z","shell.execute_reply.started":"2025-01-14T08:40:28.542772Z","shell.execute_reply":"2025-01-14T08:40:28.550784Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:40:29.236315Z","iopub.execute_input":"2025-01-14T08:40:29.236589Z","iopub.status.idle":"2025-01-14T08:40:29.242035Z","shell.execute_reply.started":"2025-01-14T08:40:29.236566Z","shell.execute_reply":"2025-01-14T08:40:29.241241Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 3453\n    })\n})"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## **Custom NER model**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass NERT5EncoderModel(nn.Module):\n    def __init__(self, encoder, num_labels):\n        super(NERT5EncoderModel, self).__init__()\n        self.encoder = encoder\n        self.classifier = nn.Linear(encoder.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state\n        logits = self.classifier(sequence_output)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:41:00.096819Z","iopub.execute_input":"2025-01-14T08:41:00.097153Z","iopub.status.idle":"2025-01-14T08:41:00.102508Z","shell.execute_reply.started":"2025-01-14T08:41:00.097127Z","shell.execute_reply":"2025-01-14T08:41:00.101444Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"num_labels = len(dataset[\"train\"].features[\"ner_tags\"].feature.names)\nmodel = NERT5EncoderModel(encoder, num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:41:00.414143Z","iopub.execute_input":"2025-01-14T08:41:00.414391Z","iopub.status.idle":"2025-01-14T08:41:00.421201Z","shell.execute_reply.started":"2025-01-14T08:41:00.414368Z","shell.execute_reply":"2025-01-14T08:41:00.420296Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:42:13.273872Z","iopub.execute_input":"2025-01-14T08:42:13.274194Z","iopub.status.idle":"2025-01-14T08:42:13.278905Z","shell.execute_reply.started":"2025-01-14T08:42:13.274170Z","shell.execute_reply":"2025-01-14T08:42:13.277329Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_dataset = tokenized_dataset[\"train\"]\ntrain_dataset.set_format(\"torch\")\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:42:14.250935Z","iopub.execute_input":"2025-01-14T08:42:14.251243Z","iopub.status.idle":"2025-01-14T08:42:14.256263Z","shell.execute_reply.started":"2025-01-14T08:42:14.251221Z","shell.execute_reply":"2025-01-14T08:42:14.255214Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:42:27.140888Z","iopub.execute_input":"2025-01-14T08:42:27.141195Z","iopub.status.idle":"2025-01-14T08:42:27.884287Z","shell.execute_reply.started":"2025-01-14T08:42:27.141172Z","shell.execute_reply":"2025-01-14T08:42:27.883293Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"NERT5EncoderModel(\n  (encoder): T5EncoderModel(\n    (shared): Embedding(32128, 512)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 512)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o): Linear(in_features=512, out_features=512, bias=False)\n                (relative_attention_bias): Embedding(32, 8)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-5): 5 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (classifier): Linear(in_features=512, out_features=9, bias=True)\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"## **Train the model**","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nfor epoch in range(10):\n    model.train()\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", leave=False)\n    \n    total_loss = 0 \n    num_batches = len(train_dataloader)\n    \n    for batch in progress_bar:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n        loss = nn.CrossEntropyLoss()(outputs.view(-1, num_labels), batch[\"labels\"].view(-1))\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        total_loss += loss.item()\n        \n        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n    \n    avg_loss = total_loss / num_batches\n    print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:42:32.986090Z","iopub.execute_input":"2025-01-14T08:42:32.986425Z","iopub.status.idle":"2025-01-14T08:46:05.792508Z","shell.execute_reply.started":"2025-01-14T08:42:32.986395Z","shell.execute_reply":"2025-01-14T08:46:05.791596Z"}},"outputs":[{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 completed. Average Loss: 1.4515\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 completed. Average Loss: 0.7484\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 completed. Average Loss: 0.5244\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 completed. Average Loss: 0.4129\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 completed. Average Loss: 0.3427\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 completed. Average Loss: 0.2974\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 completed. Average Loss: 0.2639\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 completed. Average Loss: 0.2407\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 completed. Average Loss: 0.2245\n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 10 completed. Average Loss: 0.2126\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## **Evaluation & Test**","metadata":{}},{"cell_type":"code","source":"# Tokenize the test dataset\ntest_dataset = tokenized_dataset[\"test\"]\ntest_dataset.set_format(\"torch\")\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=8,\n    collate_fn=data_collator, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:46:12.123903Z","iopub.execute_input":"2025-01-14T08:46:12.124227Z","iopub.status.idle":"2025-01-14T08:46:12.128913Z","shell.execute_reply.started":"2025-01-14T08:46:12.124200Z","shell.execute_reply":"2025-01-14T08:46:12.128158Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"test_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:46:12.819136Z","iopub.execute_input":"2025-01-14T08:46:12.819381Z","iopub.status.idle":"2025-01-14T08:46:12.824315Z","shell.execute_reply.started":"2025-01-14T08:46:12.819359Z","shell.execute_reply":"2025-01-14T08:46:12.823463Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7d2490c1ed40>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\npredictions, true_labels = [], []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n        \n        logits = outputs.detach().cpu().numpy()\n        preds = np.argmax(logits, axis=2)\n\n        predictions.extend(preds)\n        true_labels.extend(batch[\"labels\"].cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:46:15.015666Z","iopub.execute_input":"2025-01-14T08:46:15.015986Z","iopub.status.idle":"2025-01-14T08:46:19.026503Z","shell.execute_reply.started":"2025-01-14T08:46:15.015958Z","shell.execute_reply":"2025-01-14T08:46:19.025702Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nflat_predictions = [p for sublist in predictions for p in sublist]\nflat_true_labels = [l for sublist in true_labels for l in sublist]\n\nfiltered_predictions = []\nfiltered_true_labels = []\nfor pred, label in zip(flat_predictions, flat_true_labels):\n    if label != -100:  # Ignore padding tokens\n        filtered_predictions.append(pred)\n        filtered_true_labels.append(label)\n\nlabel_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n\nreport = classification_report(\n    filtered_true_labels,\n    filtered_predictions,\n    target_names=label_names,\n    zero_division=0, \n)\n\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:46:19.027404Z","iopub.execute_input":"2025-01-14T08:46:19.027603Z","iopub.status.idle":"2025-01-14T08:46:19.159538Z","shell.execute_reply.started":"2025-01-14T08:46:19.027585Z","shell.execute_reply":"2025-01-14T08:46:19.158897Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           O       0.97      1.00      0.98     38323\n       B-PER       0.91      0.89      0.90      1617\n       I-PER       0.92      0.97      0.94      1156\n       B-ORG       0.70      0.58      0.64      1661\n       I-ORG       0.72      0.50      0.59       835\n       B-LOC       0.77      0.80      0.78      1668\n       I-LOC       0.58      0.28      0.37       257\n      B-MISC       0.82      0.56      0.66       702\n      I-MISC       0.91      0.20      0.33       216\n\n    accuracy                           0.95     46435\n   macro avg       0.81      0.64      0.69     46435\nweighted avg       0.94      0.95      0.94     46435\n\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"sentence = \"Apple is looking to buy a startup in San Francisco for $1 billion.\"\n\ninputs = tokenizer(\n    sentence,\n    return_tensors=\"pt\", \n    truncation=True,      \n    is_split_into_words=False,\n)\n\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\nmodel.eval() \nwith torch.no_grad():\n    outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n    logits = outputs.detach().cpu().numpy()\n\npredicted_labels = np.argmax(logits, axis=2)\npredicted_entities = [label_names[label] for label in predicted_labels[0]]\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\nfor token, entity in zip(tokens, predicted_entities):\n    print(f\"Token: {token}, Predicted Entity: {entity}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:46:19.160995Z","iopub.execute_input":"2025-01-14T08:46:19.161227Z","iopub.status.idle":"2025-01-14T08:46:19.180710Z","shell.execute_reply.started":"2025-01-14T08:46:19.161205Z","shell.execute_reply":"2025-01-14T08:46:19.180091Z"}},"outputs":[{"name":"stdout","text":"Token: ▁Apple, Predicted Entity: B-ORG\nToken: ▁is, Predicted Entity: O\nToken: ▁looking, Predicted Entity: O\nToken: ▁to, Predicted Entity: O\nToken: ▁buy, Predicted Entity: O\nToken: ▁, Predicted Entity: O\nToken: a, Predicted Entity: O\nToken: ▁startup, Predicted Entity: O\nToken: ▁in, Predicted Entity: O\nToken: ▁San, Predicted Entity: B-LOC\nToken: ▁Francisco, Predicted Entity: I-LOC\nToken: ▁for, Predicted Entity: O\nToken: ▁$1, Predicted Entity: O\nToken: ▁billion, Predicted Entity: O\nToken: ., Predicted Entity: O\nToken: </s>, Predicted Entity: O\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
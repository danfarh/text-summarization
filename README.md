# Text Summarization Fine-Tuning on T5 Model
## About The Project
  This repository contains the code for fine-tuning the T5 model for text summarization on xsum dataset. The T5 model is a transformer-based language model developed by Google, which is known for its state-of-the-art performance on several natural language processing tasks.

## Dataset
The dataset used for this project is the XSum dataset, which is a collection of news articles and their corresponding summaries.

## Requirements
To run this code, you will need the following:

- Python 
- HuggingFace Transformers library
- PyTorch 
- Pandas
- NumPy
